{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='trainer.py')\n",
    "    parser.add_argument('--model', type=str, default='BAD', help='model to run (BAD, CLASSIFIER)')\n",
    "    parser.add_argument('--train_path', type=str, default='data/eng.train', help='path to train set (you should not need to modify)')\n",
    "    parser.add_argument('--dev_path', type=str, default='data/eng.testa', help='path to dev set (you should not need to modify)')\n",
    "    parser.add_argument('--blind_test_path', type=str, default='data/eng.testb.blind', help='path to dev set (you should not need to modify)')\n",
    "    parser.add_argument('--test_output_path', type=str, default='eng.testb.out', help='output path for test predictions')\n",
    "    parser.add_argument('--no_run_on_test', dest='run_on_test', default=True, action='store_false', help='skip printing output on the test set')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "# Wrapper for an example of the person binary classification task.\n",
    "# tokens: list of string words\n",
    "# labels: list of (0, 1) where 0 is non-name, 1 is name\n",
    "class PersonExample(object):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "# Changes NER-style chunk examples into binary classification examples.\n",
    "def transform_for_classification(ner_exs):\n",
    "    # Take each LabeledSentence object and extract the bio tags.\n",
    "    for labeled_sent in ner_exs:\n",
    "        tags = bio_tags_from_chunks(labeled_sent.chunks, len(labeled_sent))\n",
    "\n",
    "        # create a list \"labels\" that has 1 for every position with a person's name, 0 otherwise\n",
    "        labels = [1 if tag.endswith(\"PER\") else 0 for tag in tags]\n",
    "\n",
    "        # Yield a PersonExample object with a list of tokens and labels\n",
    "        yield PersonExample([tok.word for tok in labeled_sent.tokens], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountBasedPersonClassifier(object):\n",
    "    def __init__(self, pos_counts, neg_counts):\n",
    "        self.pos_counts = pos_counts\n",
    "        self.neg_counts = neg_counts\n",
    "\n",
    "    def predict(self, tokens, idx):\n",
    "        # simply checks that the \"positive\" word count is higher than the \"negative\" word count in a sentence\n",
    "        if self.pos_counts.get_count(tokens[idx]) > self.neg_counts.get_count(tokens[idx]):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"Trains\" the count-based person classifier by collecting counts over the given examples.\n",
    "def train_count_based_binary_classifier(ner_exs):\n",
    "    pos_counts = Counter()\n",
    "    neg_counts = Counter()\n",
    "    for ex in ner_exs:\n",
    "        for idx in range(0, len(ex)):\n",
    "            if ex.labels[idx] == 1:\n",
    "                pos_counts.increment_count(ex.tokens[idx], 1.0)\n",
    "            else:\n",
    "                neg_counts.increment_count(ex.tokens[idx], 1.0)\n",
    "    return CountBasedPersonClassifier(pos_counts, neg_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(exs, classifier):\n",
    "    num_correct = 0\n",
    "    num_pos_correct = 0\n",
    "    num_pred = 0\n",
    "    num_gold = 0\n",
    "    num_total = 0\n",
    "    for ex in exs:\n",
    "        for idx in range(0, len(ex)):\n",
    "            prediction = classifier.predict(ex.tokens, idx)\n",
    "            if prediction == ex.labels[idx]:\n",
    "                num_correct += 1\n",
    "            if prediction == 1:\n",
    "                num_pred += 1\n",
    "            if ex.labels[idx] == 1:\n",
    "                num_gold += 1\n",
    "            if prediction == 1 and ex.labels[idx] == 1:\n",
    "                num_pos_correct += 1\n",
    "            num_total += 1\n",
    "    print(\"Accuracy: %i / %i = %f\" % (num_correct, num_total, float(num_correct) / num_total))\n",
    "    prec = float(num_pos_correct) / num_pred if num_pred > 0 else 0.0\n",
    "    rec = float(num_pos_correct) / num_gold if num_gold > 0 else 0.0\n",
    "    f1 = 2 * prec * rec/(prec + rec) if prec > 0 and rec > 0 else 0.0\n",
    "    print(\"Precision: %i / %i = %f\" % (num_pos_correct, num_pred, prec))\n",
    "    print(\"Recall: %i / %i = %f\" % (num_pos_correct, num_gold, rec))\n",
    "    print(\"F1: %f\" % f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Runs prediction on exs and writes the outputs to outfile, one token per line\n",
    "def predict_write_output_to_file(exs, classifier, outfile):\n",
    "    f = open(outfile, 'w')\n",
    "    for ex in exs:\n",
    "        for idx in range(0, len(ex)):\n",
    "            prediction = classifier.predict(ex.tokens, idx)\n",
    "            f.write(ex.tokens[idx] + \" \" + repr(int(prediction)) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nerdata.py\n",
    "\n",
    "\n",
    "# Abstraction to bundle words with POS and chunks for featurization\n",
    "class Token:\n",
    "    def __init__(self, word, pos, chunk):\n",
    "        self.word = word\n",
    "        self.pos = pos\n",
    "        self.chunk = chunk\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Token(%s, %s, %s)\" % (self.word, self.pos, self.chunk)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "# Thin wrapper around a start and end index coupled with a label, representing,\n",
    "# e.g., a chunk PER over the span (3,5). Indices are semi-inclusive, so (3,5)\n",
    "# contains tokens 3 and 4 (0-based indexing).\n",
    "class Chunk:\n",
    "    def __init__(self, start_idx, end_idx, label):\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(\" + repr(self.start_idx) + \", \" + repr(self.end_idx) + \", \" + self.label + \")\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.start_idx == other.start_idx and self.end_idx == other.end_idx and self.label == other.label\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.start_idx) + hash(self.end_idx) + hash(self.label)\n",
    "\n",
    "\n",
    "# Thin wrapper over a sequence of Tokens representing a sentence and an optional set of chunks\n",
    "# representation NER labels, which are also stored as BIO tags\n",
    "class LabeledSentence:\n",
    "    def __init__(self, tokens, chunks=None):\n",
    "        self.tokens = tokens\n",
    "        self.chunks = chunks\n",
    "        if chunks is None:\n",
    "            self.bio_tags = None\n",
    "        else:\n",
    "            self.bio_tags = bio_tags_from_chunks(self.chunks, len(self.tokens))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr([repr(tok) for tok in self.tokens]) + \"\\n\" + repr([repr(chunk) for chunk in self.chunks])\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def get_bio_tags(self):\n",
    "        return self.bio_tags\n",
    "\n",
    "\n",
    "# We store NER tags as strings, but they contain two pieces:\n",
    "# a coarse tag type (BIO) and a label (PER), e.g. B-PER\n",
    "def isB(ner_tag):\n",
    "    return ner_tag.startswith(\"B\")\n",
    "\n",
    "\n",
    "def isI(ner_tag):\n",
    "    return ner_tag.startswith(\"I\")\n",
    "\n",
    "\n",
    "def isO(ner_tag):\n",
    "    return ner_tag == \"O\"\n",
    "\n",
    "\n",
    "# Gets the label component of the NER tag: e.g., returns PER for B-PER\n",
    "def get_tag_label(ner_tag):\n",
    "    if len(ner_tag) > 2:\n",
    "        return ner_tag[2:]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Convert BIO tags to (start, end, label) chunk representations\n",
    "# (start, end) are semi-inclusive, meaning that in the sentence\n",
    "# He met Barack Obama yesterday\n",
    "# Barack Obama has the span (2, 4)\n",
    "# N.B. this method only works because chunks are non-overlapping in this data\n",
    "def chunks_from_bio_tag_seq(bio_tags):\n",
    "    chunks = []\n",
    "    curr_tok_start = -1\n",
    "    curr_tok_label = \"\"\n",
    "    for idx, tag in enumerate(bio_tags):\n",
    "        if isB(tag):\n",
    "            label = get_tag_label(tag)\n",
    "            if curr_tok_label != \"\":\n",
    "                chunks.append(Chunk(curr_tok_start, idx, curr_tok_label))\n",
    "            curr_tok_label = label\n",
    "            curr_tok_start = idx\n",
    "        elif isI(tag):\n",
    "            label = get_tag_label(tag)\n",
    "            if label != curr_tok_label:\n",
    "                print(\"WARNING: invalid tag sequence (I after O); ignoring the I: %s\" % bio_tags)\n",
    "        else: # isO(tag):\n",
    "            if curr_tok_label != \"\":\n",
    "                chunks.append(Chunk(curr_tok_start, idx, curr_tok_label))\n",
    "            curr_tok_label = \"\"\n",
    "            curr_tok_start = -1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Converts a chunk representation back to BIO tags\n",
    "def bio_tags_from_chunks(chunks, sent_len):\n",
    "    tags = []\n",
    "    for i in range(0, sent_len):\n",
    "        matching_chunks = list(filter(lambda chunk: chunk.start_idx <= i and i < chunk.end_idx, chunks))\n",
    "        if len(matching_chunks) > 0:\n",
    "            if i == matching_chunks[0].start_idx:\n",
    "                tags.append(\"B-\" + matching_chunks[0].label)\n",
    "            else:\n",
    "                tags.append(\"I-\" + matching_chunks[0].label)\n",
    "        else:\n",
    "            tags.append(\"O\")\n",
    "    return tags\n",
    "\n",
    "\n",
    "# Reads a dataset in the CoNLL format from a file\n",
    "# The format is one token per line:\n",
    "# [word] [POS] [syntactic chunk] *potential junk column* [NER tag]\n",
    "# One blank line appears after each sentence\n",
    "def read_data(file):\n",
    "    f = open(file)\n",
    "    sentences = []\n",
    "    curr_tokens = []\n",
    "    curr_bio_tags = []\n",
    "    for line in f:\n",
    "        stripped = line.strip()\n",
    "        if stripped != \"\":\n",
    "            # split the line into tokens\n",
    "            fields = stripped.split(\" \")\n",
    "\n",
    "            # If fields contains 4 or 5 \"words\", then append the first three to curr_tokens,\n",
    "            # and append the last to curr_bio_tags\n",
    "            if len(fields) == 4 or len(fields) == 5:\n",
    "                curr_tokens.append(Token(fields[0], fields[1], fields[2]))\n",
    "                curr_bio_tags.append(fields[-1])\n",
    "\n",
    "        # If the line is empty, then the sentence is complete. Create a LabeledSentence object and\n",
    "        # add it to the list \"sentences\"\n",
    "        elif stripped == \"\" and len(curr_tokens) > 0:\n",
    "            sentences.append(LabeledSentence(curr_tokens, chunks_from_bio_tag_seq(curr_bio_tags)))\n",
    "            curr_tokens = []\n",
    "            curr_bio_tags = []\n",
    "\n",
    "    # return the list of LabeledSentence objects\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Evaluates the guess sentences with respect to the gold sentences\n",
    "def print_evaluation(gold_sentences, guess_sentences):\n",
    "    correct = 0\n",
    "    num_pred = 0\n",
    "    num_gold = 0\n",
    "    for gold, guess in zip(gold_sentences, guess_sentences):\n",
    "        correct += len(set(guess.chunks) & set(gold.chunks))\n",
    "        num_pred += len(guess.chunks)\n",
    "        num_gold += len(gold.chunks)\n",
    "    if num_pred == 0:\n",
    "        prec = 0\n",
    "    else:\n",
    "        prec = correct/float(num_pred)\n",
    "    if num_gold == 0:\n",
    "        rec = 0\n",
    "    else:\n",
    "        rec = correct/float(num_gold)\n",
    "    if prec == 0 and rec == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * prec * rec / (prec + rec)\n",
    "    print(\"Labeled F1: \" + \"{0:.2f}\".format(f1 * 100) +\\\n",
    "          \", precision: %i/%i\" % (correct, num_pred) + \" = \" + \"{0:.2f}\".format(prec * 100) + \\\n",
    "          \", recall: %i/%i\" % (correct, num_gold) + \" = \" + \"{0:.2f}\".format(rec * 100))\n",
    "\n",
    "\n",
    "# Writes labeled_sentences to outfile in the CoNLL format\n",
    "def print_output(labeled_sentences, outfile):\n",
    "    f = open(outfile, 'w')\n",
    "    for sentence in labeled_sentences:\n",
    "        bio_tags = sentence.get_bio_tags()\n",
    "        for i in range(0, len(sentence)):\n",
    "            tok = sentence.tokens[i]\n",
    "            f.write(tok.word + \" \" + tok.pos + \" \" + tok.chunk + \" \" + bio_tags[i] + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    print(\"Wrote predictions on %i labeled sentences to %s\" % (len(labeled_sentences), outfile))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adagrad_trainer.py\n",
    "\n",
    "from utils import *\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Optimizer(ABC):\n",
    "    # Scores a sparse feature vector\n",
    "    # feats: list of integer feature indices\n",
    "    def score(self, feats):\n",
    "        i = 0\n",
    "        score = 0.0\n",
    "        while i < len(feats):\n",
    "            score += self.access(feats[i])\n",
    "            i += 1\n",
    "        return score\n",
    "\n",
    "    @abstractmethod\n",
    "    def apply_gradient_update(self, gradient, batch_size):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def access(self, i):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_final_weights(self, i):\n",
    "        pass\n",
    "\n",
    "\n",
    "# SGD optimizer implementation, designed to have the same interface as the Adagrad optimizers\n",
    "class SGDOptimizer(Optimizer):\n",
    "    # init_weights: a numpy array of the correct dimension, usually initialized to 0\n",
    "    # alpha: step size\n",
    "    def __init__(self, init_weights, alpha):\n",
    "        self.weights = init_weights\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def apply_gradient_update(self, gradient, batch_size):\n",
    "        for i in gradient.keys():\n",
    "            g = gradient.get_count(i)\n",
    "            self.weights[i] = self.weights[i] + self.alpha * g\n",
    "\n",
    "    # Get the weight of feature i\n",
    "    def access(self, i):\n",
    "        return self.weights[i]\n",
    "\n",
    "    def get_final_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "\n",
    "# Wraps a weight vector and applies the Adagrad update using second moments of features to make custom step sizes.\n",
    "# This version incorporates L1 regularization: while this regularization should be applied to squash the feature vector\n",
    "# on every gradient update, we instead evaluate the regularizer lazily only when the particular feature is touched\n",
    "# (either by gradient update or by access). approximate lets you turn this off for faster access, but regularization is\n",
    "# now applied somewhat inconsistently.\n",
    "# See section 5.1 of http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf for more details\n",
    "class L1RegularizedAdagradTrainer(Optimizer):\n",
    "    # init_weights: a numpy array of the correct dimension, usually initialized to 0\n",
    "    # lamb: float lambda constant for the regularizer. Values above 0.01 will often cause all features to be zeroed out.\n",
    "    # eta: float step size. Values from 0.01 to 10 often work well.\n",
    "    # approximate: turns off gradient updates on access, only uses them when weights are written to.\n",
    "    # So regularization is applied inconsistently, but it makes things faster.\n",
    "    def __init__(self, init_weights, lamb=1e-8, eta=1.0, use_regularization=False, approximate=True):\n",
    "        self.weights = init_weights\n",
    "        self.lamb = lamb\n",
    "        self.eta = eta\n",
    "        self.use_regularization = use_regularization\n",
    "        self.approximate = approximate\n",
    "        self.curr_iter = 0\n",
    "        self.last_iter_touched = [0 for i in range(0, self.weights.shape[0])]\n",
    "        self.diag_Gt = np.zeros_like(self.weights, dtype=float)\n",
    "\n",
    "    # Take a sparse representation of the gradient and make an update, normalizing by the batch size to keep\n",
    "    # hyperparameters constant as the batch size is varied\n",
    "    # gradient: Counter\n",
    "    # batch_size: integer\n",
    "    def apply_gradient_update(self, gradient, batch_size):\n",
    "        batch_size_multiplier = 1.0 / batch_size\n",
    "        self.curr_iter += 1\n",
    "        for i in gradient.keys():\n",
    "            xti = self.weights[i]\n",
    "            # N.B.We negate the gradient here because the Adagrad formulas are all for minimizing\n",
    "            # and we're trying to maximize, so think of it as minimizing the negative of the objective\n",
    "            # which has the opposite gradient\n",
    "            # See section 5.1 in http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf for more details\n",
    "            # eta is the step size, lambda is the regularization\n",
    "            gti = -gradient.get_count(i) * batch_size_multiplier\n",
    "            old_eta_over_Htii = self.eta / (1 + np.sqrt(self.diag_Gt[i]))\n",
    "            self.diag_Gt[i] += gti * gti\n",
    "            Htii = 1 + np.sqrt(self.diag_Gt[i])\n",
    "            eta_over_Htii = self.eta / Htii\n",
    "            new_xti = xti - eta_over_Htii * gti\n",
    "            # Apply the regularizer for every iteration since touched\n",
    "            iters_since_touched = self.curr_iter - self.last_iter_touched[i]\n",
    "            self.last_iter_touched[i] = self.curr_iter\n",
    "            self.weights[i] = np.sign(new_xti) * max(0, np.abs(new_xti) - self.lamb * eta_over_Htii - (iters_since_touched - 1) * self.lamb * old_eta_over_Htii)\n",
    "\n",
    "    # Get the weight of feature i\n",
    "    def access(self, i):\n",
    "        if not self.approximate and self.last_iter_touched[i] != self.curr_iter:\n",
    "            xti = self.weights[i]\n",
    "            Htii = 1 + np.sqrt(self.diag_Gt[i])\n",
    "            eta_over_Htii = self.eta / Htii\n",
    "            iters_since_touched = self.curr_iter - self.last_iter_touched[i]\n",
    "            self.last_iter_touched[i] = self.curr_iter\n",
    "            self.weights[i] = np.sign(xti) * max(0, np.abs(xti) - iters_since_touched * self.lamb * self.eta * eta_over_Htii);\n",
    "        return self.weights[i]\n",
    "\n",
    "    # Return a numpy array containing the final weight vector values -- manually calls access to force each weight to\n",
    "    # have an updated value.\n",
    "    def get_final_weights(self):\n",
    "        for i in range(0, self.weights.shape[0]):\n",
    "            self.access(i)\n",
    "        return self.weights\n",
    "\n",
    "\n",
    "# Applies the Adagrad update with no regularization. Will be substantially faster than the L1 regularized version\n",
    "# due to less computation required to update each feature.\n",
    "class UnregularizedAdagradTrainer(Optimizer):\n",
    "    def __init__(self, init_weights, eta=1.0):\n",
    "        self.weights = init_weights\n",
    "        self.eta = eta\n",
    "        self.diag_Gt = np.zeros_like(self.weights, dtype=float)\n",
    "\n",
    "    def apply_gradient_update(self, gradient, batch_size):\n",
    "        batch_size_multiplier = 1.0 / batch_size\n",
    "        for i in gradient.keys():\n",
    "            xti = self.weights[i]\n",
    "            gti = -gradient.get_count(i) * batch_size_multiplier\n",
    "            self.diag_Gt[i] += gti * gti\n",
    "            Htii = 1 + np.sqrt(self.diag_Gt[i])\n",
    "            eta_over_Htii = self.eta / Htii\n",
    "            self.weights[i] = xti - eta_over_Htii * gti\n",
    "\n",
    "    # Get the weight of feature i\n",
    "    def access(self, i):\n",
    "        return self.weights[i]\n",
    "\n",
    "    # Return a numpy array containing the final weight vector values -- manually calls access to force each weight to\n",
    "    # have an updated value.\n",
    "    def get_final_weights(self):\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonClassifier(object):\n",
    "    def __init__(self, weights, indexer):\n",
    "        self.weights = weights\n",
    "        self.indexer = indexer\n",
    "\n",
    "    # Makes a prediction for token at position idx in the given PersonExample\n",
    "    def predict(self, tokens, idx):\n",
    "        raise Exception(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(ner_exs):\n",
    "    # Todo: Implement a training method here, follow steps below:\n",
    "    # use counter to keep track of gradient\n",
    "    # can do vector implementation, using dot product instead of looping over each element\n",
    "\n",
    "    # Probably want to implement a sigmoid (logistic regression) classifier here.\n",
    "    raise Exception(\"Implement me!\")\n",
    "    # Will need to calculate the gradient as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(weights, inputs):\n",
    "    \"\"\"Implement logistic regression here. Takes two numpy arrays, calculates their dot product,\n",
    "    and plugs it into sigmoid formula\"\"\"\n",
    "    z = np.dot(weights, inputs)\n",
    "    out = 1/(1+exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()    # saves start time for calculation of running time\n",
    "    args = _parse_args()    # _parse_args() uses argparse to determine constraints (such as model to run)\n",
    "    print(args)\n",
    "\n",
    "    # Load the training and test data\n",
    "    train_class_exs = list(transform_for_classification(read_data(args.train_path)))\n",
    "    dev_class_exs = list(transform_for_classification(read_data(args.dev_path)))\n",
    "\n",
    "    # Train the model\n",
    "    if args.model == \"BAD\":\n",
    "        classifier = train_count_based_binary_classifier(train_class_exs)\n",
    "    else:\n",
    "        classifier = train_classifier(train_class_exs)\n",
    "\n",
    "\n",
    "    print(\"Data reading and training took %f seconds\" % (time.time() - start_time))\n",
    "    # Evaluate on training, development, and test data\n",
    "    print(\"===Train accuracy===\")\n",
    "    evaluate_classifier(train_class_exs, classifier)\n",
    "    print(\"===Dev accuracy===\")\n",
    "    evaluate_classifier(dev_class_exs, classifier)\n",
    "    if args.run_on_test:\n",
    "        print(\"Running on test\")\n",
    "        test_exs = list(transform_for_classification(read_data(args.blind_test_path)))\n",
    "        predict_write_output_to_file(test_exs, classifier, args.test_output_path)\n",
    "        print(\"Wrote predictions on %i labeled sentences to %s\" % (len(test_exs), args.test_output_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL] [--train_path TRAIN_PATH]\n",
      "                             [--dev_path DEV_PATH]\n",
      "                             [--blind_test_path BLIND_TEST_PATH]\n",
      "                             [--test_output_path TEST_OUTPUT_PATH]\n",
      "                             [--no_run_on_test]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\rwest\\AppData\\Roaming\\jupyter\\runtime\\kernel-67fc0dc0-9feb-4c74-a21b-6886fc9e0706.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rwest\\.virtualenvs\\mini1-v-kswoty\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
